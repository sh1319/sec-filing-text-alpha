{"cells":[{"cell_type":"code","source":["import os, re, math, hashlib, warnings\n","from pathlib import Path\n","from datetime import datetime\n","from typing import Dict, Tuple, List\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm.auto import tqdm\n","import regex as re2\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","warnings.filterwarnings(\"ignore\")\n","pd.set_option(\"display.max_rows\", 120)\n","os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n","\n","# Paths\n","INPUT_CLEAN   = \"./final_view_cleaned.parquet\"\n","OUT_8K        = \"./features_event_8k.parquet\"\n","OUT_10KQ      = \"./features_event_10kq.parquet\"\n","CHECKPOINT_DIR= \"./_checkpoints\"\n","CACHE_DIR     = \"./_cache\"\n","FINBERT_CACHE = os.path.join(CACHE_DIR, \"finbert_cache.parquet\")\n","\n","# Settings\n","USE_GPU     = True\n","BATCH_SIZE  = 64\n","PRINT_EVERY = 20000\n","FINBERT_MODEL = os.getenv(\"FINBERT_MODEL\", \"ProsusAI/finbert\")"],"metadata":{"id":"gpCULBz1Zm3t"},"id":"gpCULBz1Zm3t","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Utils\n","def ts() -> str:\n","    return datetime.now().strftime(\"%H:%M:%S\")\n","\n","def pbar(it, **kw):\n","    return tqdm(it, dynamic_ncols=True, leave=False, mininterval=0.2, **kw)\n","\n","def ensure_dirs(*paths):\n","    for p in paths:\n","        Path(p).parent.mkdir(parents=True, exist_ok=True)\n","\n","def sha1_full(s: str) -> str:\n","    b = (s or \"\").encode(\"utf-8\", \"ignore\")\n","    return f\"{hashlib.sha1(b).hexdigest()}:{len(b)}\"\n","\n","# Tokenization\n","WORD_RE       = re2.compile(r\"(?:[A-Za-z][A-Za-z']+)\")\n","SENT_SPLIT_RE = re2.compile(r\"(?<!\\b[A-Z][a-z]\\.)(?<!\\b(?:U\\.S|Inc|Co|Ltd|Mr|Ms|Dr)\\.)[.!?]+[\\s\\n]+\")\n","\n","def tokenize_words(text: str) -> List[str]:\n","    return [m.group(0).lower() for m in WORD_RE.finditer(text or \"\")]\n","\n","def split_sentences(text: str) -> List[str]:\n","    parts = SENT_SPLIT_RE.split(text or \"\")\n","    return [p.strip() for p in parts if p and p.strip()]\n","\n","# Readability\n","VOWEL_RE = re2.compile(r\"[aeiouy]+\", re2.I)\n","\n","def syllables(word: str) -> int:\n","    w = (word or \"\").lower().strip(\"'\")\n","    if not w: return 0\n","    syl = max(1, len(VOWEL_RE.findall(w)))\n","    if w.endswith(\"e\") and syl > 1: syl -= 1\n","    return syl\n","\n","def fog_proxy(text: str) -> float:\n","    sents, toks = split_sentences(text), tokenize_words(text)\n","    nW, nS = len(toks), len(sents)\n","    if nW == 0 or nS == 0: return np.nan\n","    complex_words = sum(1 for w in toks if syllables(w) >= 3)\n","    return 0.4 * (nW/nS + 100.0 * complex_words / max(1, nW))\n","\n","# FinBERT\n","_device    = 0 if (USE_GPU and torch.cuda.is_available()) else -1\n","_tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL)\n","_model     = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)\n","_finbert   = TextClassificationPipeline(model=_model, tokenizer=_tokenizer, device=_device, top_k=None)\n","\n","_MODEL_MAX_LEN = min(max(getattr(_tokenizer, \"model_max_length\", 512), 1), 4096)\n","if _MODEL_MAX_LEN <= 0: _MODEL_MAX_LEN = 512\n","_CHUNK_TOKENS  = min(256, _MODEL_MAX_LEN // 2)\n","_CHUNK_STRIDE  = min(32, max(8, _CHUNK_TOKENS // 8))\n","\n","def chunk_by_tokens(text: str, max_tokens=_CHUNK_TOKENS, stride=_CHUNK_STRIDE) -> List[str]:\n","    enc = _tokenizer(text or \"\", add_special_tokens=False)\n","    ids = enc.get(\"input_ids\", [])\n","    if not ids: return []\n","    chunks, i, N = [], 0, len(ids)\n","    while i < N:\n","        j = min(i + max_tokens, N)\n","        chunks.append(_tokenizer.decode(ids[i:j], clean_up_tokenization_spaces=True))\n","        if j == N: break\n","        i = j - stride if j - stride > 0 else j\n","    return chunks\n","\n","def _finbert_call(text_list: List[str]):\n","    return _finbert(text_list, return_all_scores=True, truncation=True,\n","                    padding=True, max_length=_MODEL_MAX_LEN, batch_size=BATCH_SIZE)\n","\n","def _load_finbert_cache(path: str):\n","    if not Path(path).exists(): return {}\n","    try:\n","        dfc = pd.read_parquet(path)\n","        return {str(r.key):(float(r.neg),float(r.neu),float(r.pos)) for r in dfc.itertuples(index=False)}\n","    except Exception: return {}\n","\n","def _save_finbert_cache(cache, path: str):\n","    if not cache: return\n","    p = Path(path); p.parent.mkdir(parents=True, exist_ok=True)\n","    df = pd.DataFrame([(k,*v) for k,v in cache.items()], columns=[\"key\",\"neg\",\"neu\",\"pos\"])\n","    df.drop_duplicates(\"key\").to_parquet(p, index=False)\n","\n","_FINBERT_CACHE = _load_finbert_cache(FINBERT_CACHE)\n","\n","def finbert_probs_one(text: str) -> Tuple[float,float,float]:\n","    k = sha1_full(text or \"\")\n","    if k in _FINBERT_CACHE: return _FINBERT_CACHE[k]\n","    chunks = chunk_by_tokens(text or \"\")\n","    if not chunks:\n","        _FINBERT_CACHE[k] = (np.nan,np.nan,np.nan); return _FINBERT_CACHE[k]\n","    outs, neg, neu, pos, cnt = _finbert_call(chunks), 0,0,0,0\n","    for scores in outs:\n","        mp = {d[\"label\"].lower():float(d[\"score\"]) for d in scores}\n","        neg += mp.get(\"negative\",0); neu += mp.get(\"neutral\",0); pos += mp.get(\"positive\",0); cnt+=1\n","    tup = (np.nan,np.nan,np.nan) if cnt==0 else (neg/cnt, neu/cnt, pos/cnt)\n","    _FINBERT_CACHE[k] = tup; return tup\n","\n","def add_finbert_feats(df: pd.DataFrame, text_col: str, prefix: str = \"\") -> pd.DataFrame:\n","    negs, neus, poss = [], [], []\n","    for i, txt in enumerate(pbar(df[text_col].astype(str), total=len(df), desc=f\"FinBERT:{prefix}\"), 1):\n","        n,u,p = finbert_probs_one(txt)\n","        negs.append(n); neus.append(u); poss.append(p)\n","        if PRINT_EVERY and (i % PRINT_EVERY == 0):\n","            _save_finbert_cache(_FINBERT_CACHE, FINBERT_CACHE)\n","    _save_finbert_cache(_FINBERT_CACHE, FINBERT_CACHE)\n","    probs = np.column_stack([negs, neus, poss]).astype(float)\n","    polarity, intensity = probs[:,2] - probs[:,0], probs[:,2] + probs[:,0]\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        entropy = -np.nansum(np.where(probs>0, probs*np.log(np.maximum(probs,1e-12)),0), axis=1)\n","    margin = np.sort(probs, axis=1)[:,-1] - np.sort(probs, axis=1)[:,-2]\n","    out = pd.DataFrame({\n","        f\"{prefix}finbert_neg\": negs, f\"{prefix}finbert_neu\": neus, f\"{prefix}finbert_pos\": poss,\n","        f\"{prefix}fb_polarity\": polarity, f\"{prefix}fb_intensity\": intensity,\n","        f\"{prefix}fb_entropy\": entropy, f\"{prefix}fb_margin\": margin\n","    }, index=df.index)\n","    return pd.concat([df, out], axis=1)\n","\n","# Novelty\n","def tfidf_cosine_novelty(curr: str, prev: str) -> float:\n","    if not (curr or \"\").strip() or not (prev or \"\").strip(): return np.nan\n","    vect = TfidfVectorizer(min_df=1, max_df=1.0, ngram_range=(1,2))\n","    X = vect.fit_transform([prev, curr])\n","    v0, v1 = X[0].toarray()[0], X[1].toarray()[0]\n","    n0, n1 = np.linalg.norm(v0), np.linalg.norm(v1)\n","    return np.nan if n0==0 or n1==0 else 1.0 - float(np.dot(v0,v1)/(n0*n1))\n","\n","# 8-K regex & topics\n","ITEM_202_RE = re2.compile(r\"\\bitem.*2\\.0*2\\b|\\b2\\.0*2\\b\", re2.I)\n","ITEM_502_RE = re2.compile(r\"\\bitem.*5\\.0*2\\b|\\b5\\.0*2\\b\", re2.I)\n","TOPIC_PATTERNS = {\n","    \"supply_chain\": [re2.compile(r\"\\bsupply\\s+chain\\b\", re2.I), re2.compile(r\"\\blogistic(s)?\\b\", re2.I)],\n","    \"cyber\":        [re2.compile(r\"\\bcyber\\b\", re2.I), re2.compile(r\"\\bransomware\\b\", re2.I)],\n","    \"climate\":      [re2.compile(r\"\\bclimate\\b\", re2.I), re2.compile(r\"\\bemission(s)?\\b\", re2.I)]\n","}\n","\n","def count_topic_hits(text: str, patterns: List[re2.Pattern]) -> int:\n","    return sum(len(p.findall(text or \"\")) for p in patterns)\n","\n","def anatomy_topics_8k(text: str) -> Dict[str, float]:\n","    nW, denom = len(tokenize_words(text)), max(1.0, len(tokenize_words(text))/1000.0)\n","    out = dict(\n","        has_item_202_earnings=bool(ITEM_202_RE.search(text or \"\")),\n","        has_item_502_exec_chg=bool(ITEM_502_RE.search(text or \"\")),\n","    )\n","    for k, pats in TOPIC_PATTERNS.items():\n","        out[f\"topic_{k}_per1k\"] = count_topic_hits(text.lower(), pats) / denom\n","    return out\n","\n","# Features\n","def readability_feats(text: str) -> Dict[str,float]:\n","    toks, sents = tokenize_words(text), split_sentences(text)\n","    return dict(\n","        fog_proxy=fog_proxy(text),\n","        avg_words_per_sent=(len(toks)/len(sents)) if sents else np.nan,\n","        type_token_ratio=(len(set(toks))/len(toks)) if toks else np.nan,\n","        len_content_words=float(len(toks))\n","    )\n","\n","def structure_feats(content: str, rf: str) -> Dict[str,float]:\n","    n_c, n_r = len(tokenize_words(content)), len(tokenize_words(rf))\n","    return dict(risk_to_content_ratio=(n_r/n_c) if n_c else np.nan, len_rf_words=float(n_r))\n","\n","def novelty_feats_same_form(curr_rf: str, prev_rf: str) -> Dict[str,float]:\n","    return dict(novelty_rf=tfidf_cosine_novelty(curr_rf or \"\", prev_rf or \"\"))\n","\n","def timing_feats(dt: pd.Timestamp) -> Dict[str,float]:\n","    return dict(filing_dow=float(int(pd.Timestamp(dt).dayofweek))) if not pd.isna(dt) else dict(filing_dow=np.nan)\n","\n","def add_nan_flags(df: pd.DataFrame) -> pd.DataFrame:\n","    out = df.copy()\n","    for c in out.select_dtypes(include=[np.number]).columns:\n","        out[f\"{c}__was_nan\"] = out[c].isna().astype(\"int8\")\n","    for c in out.columns:\n","        if pd.api.types.is_object_dtype(out[c]): out[c] = out[c].fillna(\"\")\n","    return out\n","\n","# Builders\n","def build_features_8k(df8: pd.DataFrame) -> pd.DataFrame:\n","    if df8.empty: return pd.DataFrame(columns=[\"ticker\",\"date\",\"file_type\"])\n","    out = df8.dropna(subset=[\"ticker\",\"date\"]).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n","    out = add_finbert_feats(out, \"content\")\n","    rows = []\n","    for r in pbar(out.itertuples(index=False), total=len(out), desc=\"8-K derived\"):\n","        feat = {}\n","        feat.update(readability_feats(r.content or \"\"))\n","        feat.update(anatomy_topics_8k(r.content or \"\"))\n","        feat.update(timing_feats(r.date))\n","        rows.append(feat)\n","    fe = pd.DataFrame(rows, index=out.index)\n","    res = pd.concat([out[[\"ticker\",\"date\",\"file_type\"]], out.filter(regex=r\"^finbert_|^fb_\"), fe], axis=1)\n","    return add_nan_flags(res)\n","\n","def build_features_10kq(df10: pd.DataFrame) -> pd.DataFrame:\n","    if df10.empty: return pd.DataFrame(columns=[\"ticker\",\"date\",\"file_type\"])\n","    out = df10.dropna(subset=[\"ticker\",\"date\",\"file_type\"]).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n","    out[\"file_type\"] = out[\"file_type\"].str.lower().str.strip().replace({\"10-k\":\"10k\",\"10-q\":\"10q\",\"8-k\":\"8k\"})\n","    if \"rf\" not in out.columns: out[\"rf\"] = \"\"\n","    out = add_finbert_feats(out, \"content\")\n","    prev_rf_k, rows = {}, []\n","    for r in pbar(out.itertuples(index=False), total=len(out), desc=\"10-K/Q derived\"):\n","        feat = {}\n","        feat.update(readability_feats(r.content or \"\"))\n","        feat.update(structure_feats(r.content or \"\", r.rf or \"\"))\n","        feat.update(novelty_feats_same_form(r.rf, prev_rf_k.get((r.ticker, r.file_type), \"\")))\n","        prev_rf_k[(r.ticker, r.file_type)] = r.rf\n","        feat.update(timing_feats(r.date))\n","        rows.append(feat)\n","    fe = pd.DataFrame(rows, index=out.index)\n","    fe = pd.concat([out[[\"ticker\",\"date\",\"file_type\"]], out.filter(regex=r\"^finbert_|^fb_\"), fe], axis=1)\n","    bool_cols = [c for c in fe.columns if c.startswith(\"has_item_\")]\n","    agg = {c: \"mean\" for c in fe.columns if c not in [\"ticker\",\"date\",\"file_type\"]}\n","    for b in bool_cols: agg[b] = \"max\"\n","    fe = fe.groupby([\"ticker\",\"date\",\"file_type\"], as_index=False).agg(agg).sort_values([\"ticker\",\"date\",\"file_type\"]).reset_index(drop=True)\n","    return add_nan_flags(fe)\n"],"metadata":{"id":"ZigzccykTw5j"},"id":"ZigzccykTw5j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run\n","ensure_dirs(OUT_8K, OUT_10KQ, CHECKPOINT_DIR, CACHE_DIR, FINBERT_CACHE)\n","df = pd.read_parquet(INPUT_CLEAN)\n","df.columns = [c.lower() for c in df.columns]\n","df[\"file_type\"] = df[\"file_type\"].str.lower().str.strip().replace({\"10-k\":\"10k\",\"10-q\":\"10q\",\"8-k\":\"8k\"})\n","df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n","\n","df_8k, df_10kq = df[df[\"file_type\"].eq(\"8k\")], df[df[\"file_type\"].isin({\"10k\",\"10q\"})]\n","print(f\"[{ts()}] 8-K rows: {len(df_8k):,} | 10-K/Q rows: {len(df_10kq):,}\")\n","\n","fe8 = build_features_8k(df_8k); fe8.to_parquet(OUT_8K, index=False)\n","fe10 = build_features_10kq(df_10kq); fe10.to_parquet(OUT_10KQ, index=False)\n"],"metadata":{"id":"kOgmbaeKUf4C"},"id":"kOgmbaeKUf4C","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}